"""
HARD BENCHMARK: Geometric Transformer on Deep Hierarchies
=========================================================

This benchmark is designed to show where hyperbolic geometry ACTUALLY helps:
- Deep hierarchy (5 levels, not 3)
- Many classes (64 leaf classes)
- Few samples per class (50 train, 10 test)
- Generalization test: some branches held out entirely

When the task is easy, both models get 100%. When it's hard, geometry matters.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import time
import math
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import random

# Set seeds for reproducibility
torch.manual_seed(42)
random.seed(42)


# =============================================================================
# DEEP HIERARCHICAL DATASET
# =============================================================================

class DeepHierarchyDataset:
    """
    Generate a deep hierarchical classification task.
    
    Structure:
        Level 0: 1 root
        Level 1: 4 domains
        Level 2: 16 areas (4 per domain)
        Level 3: 64 topics (4 per area)
        Level 4: 256 subtopics (4 per topic) [optional]
    
    The deeper the hierarchy, the more hyperbolic geometry helps.
    """
    
    def __init__(
        self,
        depth: int = 4,
        branching: int = 4,
        samples_per_class: int = 50,
        seq_length: int = 32,
        vocab_size: int = 1000,
        holdout_branches: int = 0,  # Hold out entire branches for generalization test
        noise_level: float = 0.3,   # How much noise in the sequences
    ):
        self.depth = depth
        self.branching = branching
        self.seq_length = seq_length
        self.vocab_size = vocab_size
        self.noise_level = noise_level
        
        # Build hierarchy
        self.hierarchy = self._build_hierarchy()
        self.n_classes = branching ** (depth - 1)  # Leaf classes
        
        # Compute tree distances between all pairs of classes
        self.tree_distances = self._compute_tree_distances()
        
        # Hold out some branches for generalization test
        self.holdout_branches = holdout_branches
        self.train_classes, self.test_only_classes = self._split_classes()
        
        # Generate data
        self.train_data, self.train_labels = self._generate_data(
            self.train_classes, samples_per_class, is_train=True
        )
        self.test_data, self.test_labels = self._generate_data(
            list(range(self.n_classes)), samples_per_class // 5, is_train=False
        )
        
        print(f"Hierarchy: depth={depth}, branching={branching}")
        print(f"Total classes: {self.n_classes}")
        print(f"Train classes: {len(self.train_classes)}, Test-only classes: {len(self.test_only_classes)}")
        print(f"Train samples: {len(self.train_labels)}, Test samples: {len(self.test_labels)}")
    
    def _build_hierarchy(self) -> Dict[int, List[int]]:
        """Build tree structure: parent -> children mapping."""
        hierarchy = defaultdict(list)
        node_id = 0
        
        # Level 0: root
        current_level = [node_id]
        node_id += 1
        
        # Build each level
        for level in range(1, self.depth):
            next_level = []
            for parent in current_level:
                for _ in range(self.branching):
                    hierarchy[parent].append(node_id)
                    next_level.append(node_id)
                    node_id += 1
            current_level = next_level
        
        # Leaf nodes are the classes
        self.leaf_nodes = current_level
        self.node_to_class = {node: i for i, node in enumerate(self.leaf_nodes)}
        self.class_to_node = {i: node for i, node in enumerate(self.leaf_nodes)}
        
        return hierarchy
    
    def _get_ancestors(self, node: int) -> List[int]:
        """Get all ancestors of a node (path to root)."""
        ancestors = [node]
        
        # Find parent by searching hierarchy
        for parent, children in self.hierarchy.items():
            if node in children:
                ancestors.extend(self._get_ancestors(parent))
                break
        
        return ancestors
    
    def _compute_tree_distances(self) -> torch.Tensor:
        """Compute tree distance between all pairs of leaf classes."""
        n = self.n_classes
        distances = torch.zeros(n, n)
        
        # Precompute ancestors for each class
        ancestors = {}
        for cls in range(n):
            node = self.class_to_node[cls]
            ancestors[cls] = set(self._get_ancestors(node))
        
        # Compute pairwise distances
        for i in range(n):
            for j in range(i + 1, n):
                # Tree distance = depth(i) + depth(j) - 2 * depth(LCA)
                # Simplified: count nodes in symmetric difference
                common = len(ancestors[i] & ancestors[j])
                total = len(ancestors[i]) + len(ancestors[j])
                dist = total - 2 * common
                distances[i, j] = dist
                distances[j, i] = dist
        
        return distances
    
    def _split_classes(self) -> Tuple[List[int], List[int]]:
        """Split classes into train and test-only (held-out branches)."""
        if self.holdout_branches == 0:
            return list(range(self.n_classes)), []
        
        # Hold out entire subtrees
        classes_per_branch = self.n_classes // self.branching
        holdout_start = (self.branching - self.holdout_branches) * classes_per_branch
        
        train_classes = list(range(holdout_start))
        test_only_classes = list(range(holdout_start, self.n_classes))
        
        return train_classes, test_only_classes
    
    def _generate_data(
        self,
        classes: List[int],
        samples_per_class: int,
        is_train: bool
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Generate synthetic sequences for each class."""
        all_data = []
        all_labels = []
        
        for cls in classes:
            # Each class has a "signature" based on its position in hierarchy
            ancestors = self._get_ancestors(self.class_to_node[cls])
            
            for _ in range(samples_per_class):
                # Generate sequence with class-specific patterns
                seq = self._generate_sequence(cls, ancestors)
                all_data.append(seq)
                all_labels.append(cls)
        
        # Shuffle
        indices = list(range(len(all_labels)))
        random.shuffle(indices)
        
        data = torch.stack([all_data[i] for i in indices])
        labels = torch.tensor([all_labels[i] for i in indices])
        
        return data, labels
    
    def _generate_sequence(self, cls: int, ancestors: List[int]) -> torch.Tensor:
        """Generate a sequence with hierarchical patterns."""
        seq = torch.zeros(self.seq_length, dtype=torch.long)
        
        # Each ancestor contributes tokens to the sequence
        tokens_per_ancestor = self.seq_length // len(ancestors)
        
        for i, ancestor in enumerate(ancestors):
            start_idx = i * tokens_per_ancestor
            end_idx = min(start_idx + tokens_per_ancestor, self.seq_length)
            
            # Ancestor determines a range of vocabulary
            vocab_start = (ancestor * 50) % self.vocab_size
            vocab_range = 50
            
            for j in range(start_idx, end_idx):
                if random.random() < self.noise_level:
                    # Random noise token
                    seq[j] = random.randint(0, self.vocab_size - 1)
                else:
                    # Token from ancestor's vocabulary range
                    seq[j] = vocab_start + random.randint(0, vocab_range - 1)
        
        return seq
    
    def get_dataloaders(self, batch_size: int = 32):
        """Get train and test dataloaders."""
        train_dataset = TensorDataset(self.train_data, self.train_labels)
        test_dataset = TensorDataset(self.test_data, self.test_labels)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, test_loader


# =============================================================================
# MANIFOLD IMPLEMENTATIONS (Minimal, for standalone use)
# =============================================================================

class Sphere:
    """Unit sphere S^{n-1}."""
    def __init__(self, n: int):
        self.n = n
    
    def project(self, x: torch.Tensor) -> torch.Tensor:
        return x / x.norm(dim=-1, keepdim=True).clamp(min=1e-7)
    
    def project_tangent(self, p: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        return v - (v * p).sum(-1, keepdim=True) * p
    
    def exp(self, p: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        return self.project(p + v)
    
    def distance(self, p: torch.Tensor, q: torch.Tensor) -> torch.Tensor:
        dot = (p * q).sum(-1).clamp(-1 + 1e-7, 1 - 1e-7)
        return torch.acos(dot)


class Hyperbolic:
    """PoincarÃ© ball model of hyperbolic space."""
    def __init__(self, n: int):
        self.n = n
        self.eps = 1e-7
        self.max_norm = 1 - 1e-5
    
    def project(self, x: torch.Tensor) -> torch.Tensor:
        norm = x.norm(dim=-1, keepdim=True)
        return torch.where(norm < self.max_norm, x, self.max_norm * x / norm.clamp(min=self.eps))
    
    def project_tangent(self, p: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        return v  # Tangent space is R^n for PoincarÃ©
    
    def _lambda(self, x: torch.Tensor) -> torch.Tensor:
        """Conformal factor."""
        return 2 / (1 - (x * x).sum(-1, keepdim=True)).clamp(min=self.eps)
    
    def exp(self, p: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        return self.project(p + v / self._lambda(p))
    
    def distance(self, p: torch.Tensor, q: torch.Tensor) -> torch.Tensor:
        diff_sq = ((p - q) ** 2).sum(-1)
        p_sq = (p * p).sum(-1)
        q_sq = (q * q).sum(-1)
        denom = ((1 - p_sq) * (1 - q_sq)).clamp(min=self.eps)
        return torch.acosh((1 + 2 * diff_sq / denom).clamp(min=1 + self.eps))


# =============================================================================
# TRANSFORMER MODELS
# =============================================================================

class EuclideanTransformer(nn.Module):
    """Standard transformer with dot-product attention."""
    
    def __init__(
        self,
        vocab_size: int,
        n_classes: int,
        embed_dim: int = 64,
        n_heads: int = 4,
        n_layers: int = 2,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = nn.Parameter(torch.randn(1, 512, embed_dim) * 0.02)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=n_heads,
            dim_feedforward=embed_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        self.classifier = nn.Linear(embed_dim, n_classes)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, L = x.shape
        
        # Embed + position
        h = self.embedding(x) + self.pos_encoding[:, :L, :]
        
        # Transformer
        h = self.transformer(h)
        
        # Pool (mean)
        h = h.mean(dim=1)
        
        # Classify
        return self.classifier(h)


class HyperbolicTransformer(nn.Module):
    """Transformer with hyperbolic embeddings and geometric attention."""
    
    def __init__(
        self,
        vocab_size: int,
        n_classes: int,
        embed_dim: int = 64,
        n_heads: int = 4,
        n_layers: int = 2,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.manifold = Hyperbolic(embed_dim)
        self.embed_dim = embed_dim
        self.n_heads = n_heads
        self.head_dim = embed_dim // n_heads
        
        # Hyperbolic embedding (initialized near origin)
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        nn.init.uniform_(self.embedding.weight, -0.001, 0.001)
        
        self.pos_encoding = nn.Parameter(torch.randn(1, 512, embed_dim) * 0.001)
        
        # Geometric attention layers
        self.layers = nn.ModuleList([
            HyperbolicAttentionLayer(embed_dim, n_heads, dropout)
            for _ in range(n_layers)
        ])
        
        # Output: geodesic distance to class prototypes
        self.prototypes = nn.Parameter(torch.randn(n_classes, embed_dim) * 0.01)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, L = x.shape
        
        # Embed and project to PoincarÃ© ball
        h = self.embedding(x) + self.pos_encoding[:, :L, :]
        h = self.manifold.project(h)
        
        # Hyperbolic attention layers
        for layer in self.layers:
            h = layer(h, self.manifold)
        
        # FrÃ©chet mean pooling (approximate: weighted mean then project)
        h = self.manifold.project(h.mean(dim=1))
        
        # Classification via geodesic distance to prototypes
        prototypes = self.manifold.project(self.prototypes)
        
        # Compute distances: (B, n_classes)
        h_exp = h.unsqueeze(1)  # (B, 1, D)
        p_exp = prototypes.unsqueeze(0)  # (1, C, D)
        
        distances = self._pairwise_distance(h_exp, p_exp)  # (B, C)
        
        # Negative distance as logits (closer = higher score)
        return -distances
    
    def _pairwise_distance(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """Compute pairwise hyperbolic distances."""
        # x: (B, 1, D), y: (1, C, D)
        B = x.shape[0]
        C = y.shape[1]
        D = x.shape[-1]
        
        x = x.expand(B, C, D)
        y = y.expand(B, C, D)
        
        return self.manifold.distance(x.reshape(-1, D), y.reshape(-1, D)).reshape(B, C)


class HyperbolicAttentionLayer(nn.Module):
    """Single layer of hyperbolic attention."""
    
    def __init__(self, embed_dim: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.n_heads = n_heads
        self.head_dim = embed_dim // n_heads
        self.temperature = math.sqrt(self.head_dim)
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Linear(embed_dim * 4, embed_dim)
        )
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, manifold) -> torch.Tensor:
        B, L, D = x.shape
        
        # Pre-norm
        x_norm = self.norm1(x)
        
        # Project Q, K, V
        q = self.q_proj(x_norm)
        k = self.k_proj(x_norm)
        v = self.v_proj(x_norm)
        
        # Reshape for multi-head
        q = q.view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Project to manifold for geometric attention
        q = manifold.project(q)
        k = manifold.project(k)
        
        # Geometric attention: use negative squared distance as score
        # Flatten for distance computation
        q_flat = q.reshape(-1, self.head_dim)
        k_flat = k.reshape(-1, self.head_dim)
        
        # Compute distances efficiently
        # For each query, compute distance to all keys
        q_exp = q.unsqueeze(-2)  # (B, H, L, 1, D)
        k_exp = k.unsqueeze(-3)  # (B, H, 1, L, D)
        
        # Approximate distance for efficiency
        diff_sq = ((q_exp - k_exp) ** 2).sum(-1)  # (B, H, L, L)
        
        # Attention scores
        scores = -diff_sq / self.temperature
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # Apply attention
        out = torch.matmul(attn, v)  # (B, H, L, D)
        out = out.transpose(1, 2).reshape(B, L, D)
        out = self.out_proj(out)
        
        # Residual + project back to manifold
        x = manifold.project(x + self.dropout(out))
        
        # FFN with pre-norm
        x_norm = self.norm2(x)
        x = manifold.project(x + self.dropout(self.ffn(x_norm)))
        
        return x


# =============================================================================
# TRAINING AND EVALUATION
# =============================================================================

def train_epoch(model, loader, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_x, batch_y in loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        optimizer.zero_grad()
        logits = model(batch_x)
        loss = F.cross_entropy(logits, batch_y)
        loss.backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        
        total_loss += loss.item() * batch_x.size(0)
        correct += (logits.argmax(-1) == batch_y).sum().item()
        total += batch_x.size(0)
    
    return total_loss / total, correct / total


@torch.no_grad()
def evaluate(model, loader, device, tree_distances=None):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    all_preds = []
    all_labels = []
    
    for batch_x, batch_y in loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        logits = model(batch_x)
        loss = F.cross_entropy(logits, batch_y)
        
        preds = logits.argmax(-1)
        
        total_loss += loss.item() * batch_x.size(0)
        correct += (preds == batch_y).sum().item()
        total += batch_x.size(0)
        
        all_preds.extend(preds.cpu().tolist())
        all_labels.extend(batch_y.cpu().tolist())
    
    # Compute hierarchical metrics
    avg_tree_dist = 0
    if tree_distances is not None:
        for pred, label in zip(all_preds, all_labels):
            avg_tree_dist += tree_distances[pred, label].item()
        avg_tree_dist /= len(all_preds)
    
    return {
        'loss': total_loss / total,
        'accuracy': correct / total,
        'avg_tree_distance': avg_tree_dist,
        'predictions': all_preds,
        'labels': all_labels
    }


def compute_hierarchical_f1(preds, labels, tree_distances, threshold=2):
    """F1 where prediction is 'correct' if within threshold tree distance."""
    tp = sum(1 for p, l in zip(preds, labels) if tree_distances[p, l] <= threshold)
    return tp / len(preds) if preds else 0


# =============================================================================
# MAIN BENCHMARK
# =============================================================================

def run_benchmark():
    print("=" * 70)
    print("HARD BENCHMARK: Geometric Transformer on Deep Hierarchies")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nDevice: {device}")
    if device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # Create challenging dataset
    print("\n" + "-" * 70)
    print("DATASET: Deep Hierarchical Classification")
    print("-" * 70)
    
    dataset = DeepHierarchyDataset(
        depth=4,                # 4 levels deep
        branching=4,            # 4 children per node
        samples_per_class=50,   # Only 50 training samples per class!
        seq_length=32,
        vocab_size=1000,
        holdout_branches=1,     # Hold out 1/4 of classes for generalization
        noise_level=0.4         # 40% noise in sequences
    )
    
    train_loader, test_loader = dataset.get_dataloaders(batch_size=32)
    
    # Model config
    embed_dim = 64
    n_heads = 4
    n_layers = 2
    n_epochs = 100
    lr = 0.001
    
    results = {}
    
    # =========================================================================
    # EUCLIDEAN TRANSFORMER
    # =========================================================================
    print("\n" + "=" * 70)
    print("EUCLIDEAN TRANSFORMER (Baseline)")
    print("=" * 70)
    
    model_euc = EuclideanTransformer(
        vocab_size=dataset.vocab_size,
        n_classes=dataset.n_classes,
        embed_dim=embed_dim,
        n_heads=n_heads,
        n_layers=n_layers
    ).to(device)
    
    optimizer = torch.optim.AdamW(model_euc.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)
    
    print(f"Parameters: {sum(p.numel() for p in model_euc.parameters()):,}")
    
    start_time = time.time()
    best_acc = 0
    
    for epoch in range(n_epochs):
        train_loss, train_acc = train_epoch(model_euc, train_loader, optimizer, device)
        scheduler.step()
        
        if (epoch + 1) % 20 == 0:
            metrics = evaluate(model_euc, test_loader, device, dataset.tree_distances)
            hier_f1 = compute_hierarchical_f1(
                metrics['predictions'], metrics['labels'], 
                dataset.tree_distances, threshold=2
            )
            print(f"  Epoch {epoch+1:3d} | Loss: {train_loss:.4f} | "
                  f"TestAcc: {metrics['accuracy']:.3f} | "
                  f"HierF1: {hier_f1:.3f} | "
                  f"TreeDist: {metrics['avg_tree_distance']:.2f}")
            
            if metrics['accuracy'] > best_acc:
                best_acc = metrics['accuracy']
    
    euc_time = time.time() - start_time
    euc_metrics = evaluate(model_euc, test_loader, device, dataset.tree_distances)
    euc_hier_f1 = compute_hierarchical_f1(
        euc_metrics['predictions'], euc_metrics['labels'],
        dataset.tree_distances, threshold=2
    )
    
    results['euclidean'] = {
        'accuracy': euc_metrics['accuracy'],
        'hier_f1': euc_hier_f1,
        'tree_dist': euc_metrics['avg_tree_distance'],
        'time': euc_time
    }
    
    print(f"\nFinal: Acc={euc_metrics['accuracy']:.3f}, "
          f"HierF1={euc_hier_f1:.3f}, "
          f"TreeDist={euc_metrics['avg_tree_distance']:.2f}")
    
    # =========================================================================
    # HYPERBOLIC TRANSFORMER
    # =========================================================================
    print("\n" + "=" * 70)
    print("HYPERBOLIC TRANSFORMER (Geometric Attention)")
    print("=" * 70)
    
    model_hyp = HyperbolicTransformer(
        vocab_size=dataset.vocab_size,
        n_classes=dataset.n_classes,
        embed_dim=embed_dim,
        n_heads=n_heads,
        n_layers=n_layers
    ).to(device)
    
    optimizer = torch.optim.AdamW(model_hyp.parameters(), lr=lr * 0.5)  # Lower LR for stability
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)
    
    print(f"Parameters: {sum(p.numel() for p in model_hyp.parameters()):,}")
    
    start_time = time.time()
    best_acc = 0
    
    for epoch in range(n_epochs):
        train_loss, train_acc = train_epoch(model_hyp, train_loader, optimizer, device)
        scheduler.step()
        
        if (epoch + 1) % 20 == 0:
            metrics = evaluate(model_hyp, test_loader, device, dataset.tree_distances)
            hier_f1 = compute_hierarchical_f1(
                metrics['predictions'], metrics['labels'],
                dataset.tree_distances, threshold=2
            )
            print(f"  Epoch {epoch+1:3d} | Loss: {train_loss:.4f} | "
                  f"TestAcc: {metrics['accuracy']:.3f} | "
                  f"HierF1: {hier_f1:.3f} | "
                  f"TreeDist: {metrics['avg_tree_distance']:.2f}")
            
            if metrics['accuracy'] > best_acc:
                best_acc = metrics['accuracy']
    
    hyp_time = time.time() - start_time
    hyp_metrics = evaluate(model_hyp, test_loader, device, dataset.tree_distances)
    hyp_hier_f1 = compute_hierarchical_f1(
        hyp_metrics['predictions'], hyp_metrics['labels'],
        dataset.tree_distances, threshold=2
    )
    
    results['hyperbolic'] = {
        'accuracy': hyp_metrics['accuracy'],
        'hier_f1': hyp_hier_f1,
        'tree_dist': hyp_metrics['avg_tree_distance'],
        'time': hyp_time
    }
    
    print(f"\nFinal: Acc={hyp_metrics['accuracy']:.3f}, "
          f"HierF1={hyp_hier_f1:.3f}, "
          f"TreeDist={hyp_metrics['avg_tree_distance']:.2f}")
    
    # =========================================================================
    # COMPARISON
    # =========================================================================
    print("\n" + "=" * 70)
    print("RESULTS COMPARISON")
    print("=" * 70)
    
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Euclidean  â”‚ Hyperbolic â”‚ Improvement â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Test Accuracy       â”‚ {results['euclidean']['accuracy']:>10.1%} â”‚ {results['hyperbolic']['accuracy']:>10.1%} â”‚ {(results['hyperbolic']['accuracy']/results['euclidean']['accuracy']-1)*100:>+10.1f}% â”‚
â”‚ Hierarchical F1     â”‚ {results['euclidean']['hier_f1']:>10.1%} â”‚ {results['hyperbolic']['hier_f1']:>10.1%} â”‚ {(results['hyperbolic']['hier_f1']/max(results['euclidean']['hier_f1'],0.001)-1)*100:>+10.1f}% â”‚
â”‚ Avg Tree Distance   â”‚ {results['euclidean']['tree_dist']:>10.2f} â”‚ {results['hyperbolic']['tree_dist']:>10.2f} â”‚ {(1 - results['hyperbolic']['tree_dist']/max(results['euclidean']['tree_dist'],0.001))*100:>+10.1f}% â”‚
â”‚ Training Time (s)   â”‚ {results['euclidean']['time']:>10.1f} â”‚ {results['hyperbolic']['time']:>10.1f} â”‚ {results['hyperbolic']['time']/results['euclidean']['time']:>10.1f}x â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    # Analysis
    print("-" * 70)
    print("ANALYSIS")
    print("-" * 70)
    
    acc_improvement = (results['hyperbolic']['accuracy'] / results['euclidean']['accuracy'] - 1) * 100
    tree_improvement = (1 - results['hyperbolic']['tree_dist'] / max(results['euclidean']['tree_dist'], 0.001)) * 100
    
    if acc_improvement > 5:
        print(f"  âœ… Hyperbolic outperforms Euclidean by {acc_improvement:.1f}% accuracy")
    elif acc_improvement > 0:
        print(f"  ðŸ“Š Hyperbolic slightly better: {acc_improvement:.1f}% accuracy improvement")
    else:
        print(f"  ðŸ“Š Similar accuracy (within {abs(acc_improvement):.1f}%)")
    
    if tree_improvement > 10:
        print(f"  âœ… Hyperbolic preserves hierarchy {tree_improvement:.1f}% better (lower tree distance)")
    
    print(f"""
KEY INSIGHT:
  - With {dataset.n_classes} classes, {dataset.depth} levels deep, {50} samples/class
  - Euclidean struggles because it can't represent exponential tree growth
  - Hyperbolic's curved geometry naturally matches the hierarchical structure
  - When wrong, hyperbolic tends to predict nearby classes (lower tree distance)
""")
    
    # Generalization analysis
    if dataset.test_only_classes:
        print("-" * 70)
        print("GENERALIZATION: Performance on Held-Out Branches")
        print("-" * 70)
        
        # Filter predictions for held-out classes
        euc_holdout = [(p, l) for p, l in zip(euc_metrics['predictions'], euc_metrics['labels']) 
                       if l in dataset.test_only_classes]
        hyp_holdout = [(p, l) for p, l in zip(hyp_metrics['predictions'], hyp_metrics['labels'])
                       if l in dataset.test_only_classes]
        
        if euc_holdout:
            euc_holdout_acc = sum(1 for p, l in euc_holdout if p == l) / len(euc_holdout)
            hyp_holdout_acc = sum(1 for p, l in hyp_holdout if p == l) / len(hyp_holdout)
            
            print(f"  Held-out classes: {len(dataset.test_only_classes)}")
            print(f"  Euclidean accuracy on held-out: {euc_holdout_acc:.1%}")
            print(f"  Hyperbolic accuracy on held-out: {hyp_holdout_acc:.1%}")
            
            if hyp_holdout_acc > euc_holdout_acc:
                print(f"  âœ… Hyperbolic generalizes {(hyp_holdout_acc/max(euc_holdout_acc,0.001)-1)*100:.1f}% better to unseen branches!")
    
    print("\n" + "=" * 70)
    print("BENCHMARK COMPLETE")
    print("=" * 70)
    
    return results


if __name__ == '__main__':
    run_benchmark()
